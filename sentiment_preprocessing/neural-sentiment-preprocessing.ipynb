{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45df197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7994107d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "(50000, 2)\n",
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Download necessary data\n",
    "import spacy.cli\n",
    "spacy.cli.download('en_core_web_sm') # Download English model\n",
    "\n",
    "# Load the IMDB dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c895e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the raw data\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])  # Load spacy without unnecessary components (for this project) for efficiency\n",
    "\n",
    "# Apply preprocessing using batch processing for speed\n",
    "docs = list(nlp.pipe(df['review'], batch_size=1000))\n",
    "\n",
    "# Create cleaned reviews by lemmatizing and removing stopwords and punctuation\n",
    "df['cleaned_review'] = [' '.join([token.lemma_ for token in doc if not token.is_stop and (token.is_alpha or token.is_digit)]) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f97ba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      cleaned_review  \\\n",
      "0  reviewer mention watch 1 Oz episode hook right...   \n",
      "1  wonderful little production br filming techniq...   \n",
      "2  think wonderful way spend time hot summer week...   \n",
      "3  basically family little boy Jake think zombie ...   \n",
      "4  Petter Mattei love Time money visually stunnin...   \n",
      "\n",
      "                                             bigrams  \n",
      "0  [(reviewer, mention), (mention, watch), (watch...  \n",
      "1  [(wonderful, little), (little, production), (p...  \n",
      "2  [(think, wonderful), (wonderful, way), (way, s...  \n",
      "3  [(basically, family), (family, little), (littl...  \n",
      "4  [(Petter, Mattei), (Mattei, love), (love, Time...  \n"
     ]
    }
   ],
   "source": [
    "# Generate n-grams from cleaned text\n",
    "def generate_ngrams(text, n=2):\n",
    "    words = text.split()\n",
    "    return list(ngrams(words, n))\n",
    "\n",
    "# Apply n-gram generation (bigrams)\n",
    "df['bigrams'] = df['cleaned_review'].apply(lambda x: generate_ngrams(x, 2))\n",
    "print(df[['cleaned_review', 'bigrams']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca0ea374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to 'vocabulary.pkl'\n",
      "Vocabulary Size (including special tokens): 106999\n",
      "First 10 words in vocabulary: ['<PAD>', '<UNK>', '<SOS>', '<EOS>', '0', '00', '000', '001', '006', '0069']\n",
      "Last 10 words: ['úber', 'über', 'übermensch', 'übermenschlich', 'überwoman', 'ünfaithful', 'ý', 'ýs', 'יגאל', 'כרמון']\n",
      "Example mapping: \"the\" -> 100562\n"
     ]
    }
   ],
   "source": [
    "# Build advanced vocabulary with special tokens and indexing\n",
    "def build_vocabulary(reviews):\n",
    "    special_tokens = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']  # Padding, Unknown, Start of Sequence, End of Sequence\n",
    "    all_words = []\n",
    "    for review in reviews:\n",
    "        all_words.extend(review.split()) # Split reviews into words and collect them\n",
    "    vocabulary = special_tokens + sorted(set(all_words)) # Create vocabulary list\n",
    "    \n",
    "    # Create word-to-index and index-to-word mappings (using dictionary comprehensions)\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)} # Map words to indices (encode)\n",
    "    index_to_word = {idx: word for word, idx in word_to_index.items()} # Map indices to words (decode)\n",
    "    \n",
    "    return vocabulary, word_to_index, index_to_word\n",
    "\n",
    "vocabulary, word_to_index, index_to_word = build_vocabulary(df['cleaned_review'])\n",
    "\n",
    "# Save vocabulary and mappings to a pickle file\n",
    "with open('vocabulary.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'vocabulary': vocabulary,\n",
    "        'word_to_index': word_to_index,\n",
    "        'index_to_word': index_to_word\n",
    "    }, f)\n",
    "\n",
    "print(\"Vocabulary saved to 'vocabulary.pkl'\")\n",
    "\n",
    "print(f'Vocabulary Size (including special tokens): {len(vocabulary)}')\n",
    "print(f'First 10 words in vocabulary: {vocabulary[:10]}')\n",
    "print(f'Last 10 words: {vocabulary[-10:]}')\n",
    "print(f'Example mapping: \"the\" -> {word_to_index.get(\"the\", word_to_index[\"<UNK>\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8aa1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      cleaned_review  \\\n",
      "0  reviewer mention watch 1 Oz episode hook right...   \n",
      "1  wonderful little production br filming techniq...   \n",
      "2  think wonderful way spend time hot summer week...   \n",
      "3  basically family little boy Jake think zombie ...   \n",
      "4  Petter Mattei love Time money visually stunnin...   \n",
      "\n",
      "                                           sequences  \n",
      "0  [93201, 83816, 105321, 34, 38095, 71281, 77552...  \n",
      "1  [106175, 82146, 90597, 61928, 73016, 100191, 8...  \n",
      "2  [100691, 106175, 105401, 97539, 100951, 77703,...  \n",
      "3  [60331, 72418, 82146, 61912, 25856, 100691, 10...  \n",
      "4  [39567, 32996, 82479, 51418, 84734, 104876, 98...  \n",
      "Sequences saved to 'sequences.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# Convert cleaned reviews to sequences (vectors) using word-to-index mapping\n",
    "df['sequences'] = df['cleaned_review'].apply(lambda x: [word_to_index.get(word, word_to_index['<UNK>']) for word in x.split()])\n",
    "\n",
    "# Display sample sequences\n",
    "print(df[['cleaned_review', 'sequences']].head())\n",
    "\n",
    "# Save sequences and labels to a pickle file for model training\n",
    "sequences_data = {\n",
    "    'sequences': df['sequences'].tolist(),\n",
    "    'labels': df['sentiment'].map({'positive': 1, 'negative': 0}).tolist()  # Assuming 'sentiment' column has 'positive'/'negative'\n",
    "}\n",
    "with open('sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(sequences_data, f)\n",
    "\n",
    "print(\"Sequences saved to 'sequences.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38822613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
