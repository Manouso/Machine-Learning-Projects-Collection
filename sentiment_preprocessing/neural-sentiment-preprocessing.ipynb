{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45df197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7994107d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "(50000, 2)\n",
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Download necessary data\n",
    "import spacy.cli\n",
    "spacy.cli.download('en_core_web_sm') # Download English model\n",
    "\n",
    "# Load the IMDB dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e4e3b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "2  I thought this was a wonderful way to spend ti...   \n",
      "3  Basically there's a family where a little boy ...   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  reviewer mention watch 1 Oz episode hook right...  \n",
      "1  wonderful little production br filming techniq...  \n",
      "2  think wonderful way spend time hot summer week...  \n",
      "3  basically family little boy Jake think zombie ...  \n",
      "4  Petter Mattei love Time money visually stunnin...  \n"
     ]
    }
   ],
   "source": [
    "# Clean and preprocess the raw data\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])  # Load spacy without unnecessary components (for this project) for efficiency\n",
    "\n",
    "# Preprocessing function to clean reviews\n",
    "def preprocess_dataset(texts):\n",
    "    cleaned_texts = []\n",
    "    docs = list(nlp.pipe(texts, batch_size=1000))\n",
    "    for doc in docs:\n",
    "        # Collect lemmas for valid tokens in this doc\n",
    "        cleaned = [token.lemma_ for token in doc if not token.is_stop and (token.is_alpha or token.is_digit)]\n",
    "        # Join into a single string per review\n",
    "        cleaned_texts.append(' '.join(cleaned))\n",
    "    return cleaned_texts\n",
    "\n",
    "# Apply preprocessing to the reviews\n",
    "df['cleaned_review'] = preprocess_dataset(df['review'])\n",
    "print(df[['review', 'cleaned_review']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f97ba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      cleaned_review  \\\n",
      "0  reviewer mention watch 1 Oz episode hook right...   \n",
      "1  wonderful little production br filming techniq...   \n",
      "2  think wonderful way spend time hot summer week...   \n",
      "3  basically family little boy Jake think zombie ...   \n",
      "4  Petter Mattei love Time money visually stunnin...   \n",
      "\n",
      "                                             bigrams  \n",
      "0  [(reviewer, mention), (mention, watch), (watch...  \n",
      "1  [(wonderful, little), (little, production), (p...  \n",
      "2  [(think, wonderful), (wonderful, way), (way, s...  \n",
      "3  [(basically, family), (family, little), (littl...  \n",
      "4  [(Petter, Mattei), (Mattei, love), (love, Time...  \n"
     ]
    }
   ],
   "source": [
    "# Generate n-grams from cleaned text\n",
    "def generate_ngrams(text, n=2):\n",
    "    words = text.split()\n",
    "    return list(ngrams(words, n))\n",
    "\n",
    "# Apply n-gram generation (bigrams)\n",
    "df['bigrams'] = df['cleaned_review'].apply(lambda x: generate_ngrams(x, 2))\n",
    "print(df[['cleaned_review', 'bigrams']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278a5877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "Neural preprocessing complete. Data saved for model training.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained GloVe word embeddings\n",
    "model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Prepare data for neural network using pretrained GloVe embeddings\n",
    "\n",
    "# Use cleaned_review without special tokens\n",
    "df['label'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Define special tokens for sequence modeling\n",
    "special_tokens = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "\n",
    "# Get all unique words\n",
    "all_words = set()\n",
    "for review in df['cleaned_review']:\n",
    "    words = review.split()\n",
    "    all_words.update(words)\n",
    "\n",
    "# The vocabulary contains all the necessary words and tokens for the embeddings\n",
    "vocab = special_tokens + list(all_words)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Every word should be assigned with an index for vectorization \n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_dim = 100\n",
    "# vocab size (x) x embedding dimensions (y) \n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# The embedding matrix has all the words that exist in both the vocabulary of the dataset and pre-trained model\n",
    "for word, i in word_to_index.items():\n",
    "    if word in model and word not in special_tokens:\n",
    "        # Returns vector with 100 values\n",
    "        embedding_matrix[i] = model[word]\n",
    "\n",
    "# Convert to sequences with <SOS> and <EOS>\n",
    "X = []\n",
    "for review in df['cleaned_review']:\n",
    "    # Assigns index of SOS to begin and if a word doesnt exist it assigns index of UNK and in the end EOS \n",
    "    seq = [word_to_index['<SOS>']] + [word_to_index.get(word, word_to_index['<UNK>']) for word in review.split()] + [word_to_index['<EOS>']]\n",
    "    X.append(seq)\n",
    "\n",
    "# Pad sequences with <PAD> (index 0)\n",
    "max_len = max(len(seq) for seq in X)\n",
    "X = np.array([seq + [word_to_index['<PAD>']] * (max_len - len(seq)) for seq in X])\n",
    "\n",
    "y = df['label'].values\n",
    "\n",
    "# Save preprocessing artifacts\n",
    "with open('neural_preprocessing.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'word_to_index': word_to_index,\n",
    "        'embedding_matrix': embedding_matrix,\n",
    "        'max_len': max_len,\n",
    "        'X': X,\n",
    "        'y': df['label'].values\n",
    "    }, f)\n",
    "\n",
    "print(\"Neural preprocessing complete. Data saved for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74993f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split complete.\n",
      "Training set size: 40000\n",
      "Test set size: 10000\n",
      "Input shape: (40000, 1298)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "print(\"Data split complete.\")\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Input shape: {X_train.shape}\")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.from_numpy(X_train).long() \n",
    "y_train = torch.from_numpy(y_train.values).float()\n",
    "X_test = torch.from_numpy(X_test).long()\n",
    "y_test = torch.from_numpy(y_test.values).float()\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e7f9107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 0.3693, Val Loss 0.3037, Acc 0.8747, Prec 0.8369, Rec 0.9308\n",
      "Epoch 2: Train Loss 0.2197, Val Loss 0.2756, Acc 0.8892, Prec 0.8927, Rec 0.8848\n",
      "Epoch 3: Train Loss 0.1406, Val Loss 0.3230, Acc 0.8851, Prec 0.8722, Rec 0.9024\n",
      "Epoch 4: Train Loss 0.0895, Val Loss 0.4016, Acc 0.8794, Prec 0.8743, Rec 0.8862\n",
      "Epoch 5: Train Loss 0.0569, Val Loss 0.5096, Acc 0.8699, Prec 0.8908, Rec 0.8432\n",
      "Final Test Acc: 0.8892, Prec: 0.8927, Rec: 0.8848\n"
     ]
    }
   ],
   "source": [
    "# Neural Model for Sentimantic Analysis\n",
    "\n",
    "class SemanticClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure embedding_matrix is a tensor\n",
    "        weights = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights, freeze=False) # freeze = False allows fine-tuning\n",
    "        self.fc1 = nn.Linear(weights.shape[1], hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        # Create mask for non-padding tokens\n",
    "        mask = (x!=0).unsqueeze(-1).float()\n",
    "        # Apply mask to embeddings\n",
    "        masked_embedded = embedded * mask # Zero out padding embeddings\n",
    "        # Masked average pooling to aggregate the sequence of embeddings into a single fixed-size vector per sample \n",
    "        pooled = masked_embedded.sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "        output = torch.relu(self.bn1(self.fc1(pooled)))\n",
    "        output = self.dropout(output)\n",
    "        # Return RAW logits (No sigmoid here!)\n",
    "        return self.fc2(output).squeeze()\n",
    "    \n",
    "# Hyperparameters for the model (optimizer,regularization,learning_rate)\n",
    "\n",
    "model = SemanticClassifier(embedding_matrix, hidden_dim=256, output_dim=1, dropout=0.5)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss() # More stable\n",
    "patience = 0\n",
    "\n",
    "# Training with 20 epochs\n",
    "epochs = 20\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            val_outputs = model(batch_X)\n",
    "            loss = criterion(val_outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            preds = (val_outputs > 0).int()  # Note: > 0 since logits, not probs\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_labels.extend(batch_y.int().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss {train_loss/len(train_loader):.4f}, Val Loss {val_loss/len(test_loader):.4f}, Acc {acc:.4f}, Prec {prec:.4f}, Rec {rec:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        patience = 0 \n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= 3:\n",
    "            break\n",
    "\n",
    "\n",
    "# Load best and evaluate\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_final_preds = []\n",
    "    all_final_labels = []\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        final_outputs = model(batch_X)\n",
    "        final_preds = (final_outputs > 0).int()\n",
    "        all_final_preds.extend(final_preds.numpy())\n",
    "        all_final_labels.extend(batch_y.int().numpy())\n",
    "    \n",
    "    final_acc = accuracy_score(all_final_labels, all_final_preds)\n",
    "    final_prec = precision_score(all_final_labels, all_final_preds, zero_division=0)\n",
    "    final_rec = recall_score(all_final_labels, all_final_preds, zero_division=0)\n",
    "    print(f\"Final Test Acc: {final_acc:.4f}, Prec: {final_prec:.4f}, Rec: {final_rec:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
