{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45df197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7994107d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "(50000, 2)\n",
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Download necessary data\n",
    "import spacy.cli\n",
    "spacy.cli.download('en_core_web_sm') # Download English model\n",
    "\n",
    "# Load the IMDB dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e4e3b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "2  I thought this was a wonderful way to spend ti...   \n",
      "3  Basically there's a family where a little boy ...   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  reviewer mention watch 1 Oz episode hook right...  \n",
      "1  wonderful little production br filming techniq...  \n",
      "2  think wonderful way spend time hot summer week...  \n",
      "3  basically family little boy Jake think zombie ...  \n",
      "4  Petter Mattei love Time money visually stunnin...  \n"
     ]
    }
   ],
   "source": [
    "# Clean and preprocess the raw data\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])  # Load spacy without unnecessary components (for this project) for efficiency\n",
    "\n",
    "# Preprocessing function to clean reviews\n",
    "def preprocess_dataset(texts):\n",
    "    cleaned_texts = []\n",
    "    docs = list(nlp.pipe(texts, batch_size=1000))\n",
    "    for doc in docs:\n",
    "        # Collect lemmas for valid tokens in this doc\n",
    "        cleaned = [token.lemma_ for token in doc if not token.is_stop and (token.is_alpha or token.is_digit)]\n",
    "        # Join into a single string per review\n",
    "        cleaned_texts.append(' '.join(cleaned))\n",
    "    return cleaned_texts\n",
    "\n",
    "# Apply preprocessing to the reviews\n",
    "df['cleaned_review'] = preprocess_dataset(df['review'])\n",
    "print(df[['review', 'cleaned_review']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f97ba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      cleaned_review  \\\n",
      "0  reviewer mention watch 1 Oz episode hook right...   \n",
      "1  wonderful little production br filming techniq...   \n",
      "2  think wonderful way spend time hot summer week...   \n",
      "3  basically family little boy Jake think zombie ...   \n",
      "4  Petter Mattei love Time money visually stunnin...   \n",
      "\n",
      "                                             bigrams  \n",
      "0  [(reviewer, mention), (mention, watch), (watch...  \n",
      "1  [(wonderful, little), (little, production), (p...  \n",
      "2  [(think, wonderful), (wonderful, way), (way, s...  \n",
      "3  [(basically, family), (family, little), (littl...  \n",
      "4  [(Petter, Mattei), (Mattei, love), (love, Time...  \n"
     ]
    }
   ],
   "source": [
    "# Generate n-grams from cleaned text\n",
    "def generate_ngrams(text, n=2):\n",
    "    words = text.split()\n",
    "    return list(ngrams(words, n))\n",
    "\n",
    "# Apply n-gram generation (bigrams)\n",
    "df['bigrams'] = df['cleaned_review'].apply(lambda x: generate_ngrams(x, 2))\n",
    "print(df[['cleaned_review', 'bigrams']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278a5877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "Neural preprocessing complete. Data saved for model training.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained GloVe word embeddings\n",
    "model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Prepare data for neural network using pretrained GloVe embeddings\n",
    "\n",
    "# Use cleaned_review without special tokens\n",
    "df['label'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Define special tokens for sequence modeling\n",
    "special_tokens = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "\n",
    "# Get all unique words\n",
    "all_words = set()\n",
    "for review in df['cleaned_review']:\n",
    "    words = review.split()\n",
    "    all_words.update(words)\n",
    "\n",
    "# The vocabulary contains all the necessary words and tokens for the embeddings\n",
    "vocab = special_tokens + list(all_words)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Every word should be assigned with an index for vectorization \n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_dim = 100\n",
    "# vocab size (x) x embedding dimensions (y) \n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# The embedding matrix has all the words that exist in both the vocabulary of the dataset and pre-trained model\n",
    "for word, i in word_to_index.items():\n",
    "    if word in model and word not in special_tokens:\n",
    "        # Returns vector with 100 values\n",
    "        embedding_matrix[i] = model[word]\n",
    "\n",
    "# Convert to sequences with <SOS> and <EOS>\n",
    "X = []\n",
    "for review in df['cleaned_review']:\n",
    "    # Assigns index of SOS to begin and if a word doesnt exist it assigns index of UNK and in the end EOS \n",
    "    seq = [word_to_index['<SOS>']] + [word_to_index.get(word, word_to_index['<UNK>']) for word in review.split()] + [word_to_index['<EOS>']]\n",
    "    X.append(seq)\n",
    "\n",
    "# Pad sequences with <PAD> (index 0)\n",
    "max_len = max(len(seq) for seq in X)\n",
    "X = np.array([seq + [word_to_index['<PAD>']] * (max_len - len(seq)) for seq in X])\n",
    "\n",
    "y = df['label'].values\n",
    "\n",
    "# Save preprocessing artifacts\n",
    "with open('neural_preprocessing.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'word_to_index': word_to_index,\n",
    "        'embedding_matrix': embedding_matrix,\n",
    "        'max_len': max_len,\n",
    "        'X': X,\n",
    "        'y': df['label'].values\n",
    "    }, f)\n",
    "\n",
    "print(\"Neural preprocessing complete. Data saved for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b74993f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split complete.\n",
      "Training set size: 40000\n",
      "Test set size: 10000\n",
      "Input shape: (40000, 1298)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "print(\"Data split complete.\")\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Input shape: {X_train.shape}\")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.from_numpy(X_train).long() \n",
    "y_train = torch.from_numpy(y_train.values).float()\n",
    "X_test = torch.from_numpy(X_test).long()\n",
    "y_test = torch.from_numpy(y_test.values).float()\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e7f9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Model for Sentimantic Analysis\n",
    "class SemanticClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        weights = torch.as_tensor(embedding_matrix, dtype=torch.float)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights, freeze=False)\n",
    "        \n",
    "        # Concatenated Mean + Max pooling = 2 * embedding_dim\n",
    "        input_dim = weights.shape[1] * 2 \n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(), # Better gradient flow than ReLU\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        mask = (x != 0).unsqueeze(-1).float()\n",
    "        \n",
    "        # Masked Mean Pooling\n",
    "        mean_pooled = (embedded * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "        # Masked Max Pooling (set padding to very low value)\n",
    "        max_pooled, _ = torch.max(embedded * mask + (1 - mask) * -1e9, dim=1)\n",
    "        \n",
    "        # Combine both to capture average sentiment AND peak signals\n",
    "        combined = torch.cat([mean_pooled, max_pooled], dim=1)\n",
    "        return self.net(combined).squeeze()\n",
    "\n",
    "# Implement GRU and LSTM to compare them with the other neural and classical approaches\n",
    "\n",
    "# Optimized RNN Base (Used for both GRU and LSTM)\n",
    "class OptimizedRNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout, rnn_type=\"GRU\"):\n",
    "        super().__init__()\n",
    "        weights = torch.as_tensor(embedding_matrix, dtype=torch.float)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights, freeze=False)\n",
    "        \n",
    "        # Added Bidirectionality (looks at context from both ends)\n",
    "        # Added Multiple Layers\n",
    "        RNN_Class = nn.GRU if rnn_type == \"GRU\" else nn.LSTM\n",
    "        self.rnn = RNN_Class(\n",
    "            weights.shape[1], \n",
    "            hidden_dim, \n",
    "            num_layers=1, \n",
    "            bidirectional=True, \n",
    "            batch_first=True, \n",
    "            dropout=0\n",
    "        )\n",
    "        \n",
    "        # Input is hidden_dim * 2 (bidirectional)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Use torch.sum on current device\n",
    "        lengths = (x != 0).sum(dim=1).to(torch.int64).cpu() \n",
    "        \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Use full output instead of just hidden state\n",
    "        packed_output, _ = self.rnn(packed)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Global Max Pooling over time: Captures the most important feature found in the sequence\n",
    "        pooled, _ = torch.max(output, dim=1)\n",
    "        \n",
    "        return self.fc(pooled).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GRU approach (faster, simpler)\n",
    "class SemanticClassifierGRU(OptimizedRNN):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout):\n",
    "        super().__init__(embedding_matrix, hidden_dim, output_dim, dropout, rnn_type=\"GRU\")\n",
    "\n",
    "# LSTM approach (more reliable)\n",
    "class SemanticClassifierLSTM(OptimizedRNN):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout):\n",
    "        super().__init__(embedding_matrix, hidden_dim, output_dim, dropout, rnn_type=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "178f0ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SemanticClassifier...\n",
      "Epoch 1: Train Loss 0.5154, Val Loss 0.4653, Acc 0.7775, Prec 0.8494, Rec 0.6746, F1 0.7520\n",
      "Epoch 2: Train Loss 0.4672, Val Loss 0.4389, Acc 0.7946, Prec 0.7581, Rec 0.8652, F1 0.8081\n",
      "Epoch 3: Train Loss 0.4570, Val Loss 0.4252, Acc 0.8024, Prec 0.8149, Rec 0.7826, F1 0.7984\n",
      "Epoch 4: Train Loss 0.4514, Val Loss 0.4250, Acc 0.8052, Prec 0.8122, Rec 0.7940, F1 0.8030\n",
      "Epoch 5: Train Loss 0.4471, Val Loss 0.4291, Acc 0.7997, Prec 0.8373, Rec 0.7440, F1 0.7879\n",
      "Embeddings unfrozen.\n",
      "Epoch 6: Train Loss 0.3480, Val Loss 0.2744, Acc 0.8859, Prec 0.8931, Rec 0.8768, F1 0.8849\n",
      "Epoch 7: Train Loss 0.2227, Val Loss 0.2721, Acc 0.8906, Prec 0.8900, Rec 0.8914, F1 0.8907\n",
      "Epoch 8: Train Loss 0.1528, Val Loss 0.3141, Acc 0.8831, Prec 0.8813, Rec 0.8854, F1 0.8834\n",
      "Epoch 9: Train Loss 0.1082, Val Loss 0.4014, Acc 0.8763, Prec 0.8531, Rec 0.9092, F1 0.8802\n",
      "Epoch 10: Train Loss 0.0747, Val Loss 0.4661, Acc 0.8730, Prec 0.8624, Rec 0.8876, F1 0.8748\n",
      "Epoch 11: Train Loss 0.0537, Val Loss 0.5386, Acc 0.8659, Prec 0.8828, Rec 0.8438, F1 0.8629\n",
      "Epoch 12: Train Loss 0.0479, Val Loss 0.6601, Acc 0.8607, Prec 0.8305, Rec 0.9064, F1 0.8668\n",
      "\n",
      "Training SemanticClassifierGRU...\n",
      "Epoch 1: Train Loss 0.4073, Val Loss 0.3705, Acc 0.8282, Prec 0.7620, Rec 0.9546, F1 0.8475\n",
      "Epoch 2: Train Loss 0.3356, Val Loss 0.2981, Acc 0.8718, Prec 0.8756, Rec 0.8668, F1 0.8712\n",
      "Epoch 3: Train Loss 0.3041, Val Loss 0.3016, Acc 0.8699, Prec 0.8948, Rec 0.8384, F1 0.8657\n",
      "Epoch 4: Train Loss 0.2803, Val Loss 0.2997, Acc 0.8710, Prec 0.8969, Rec 0.8384, F1 0.8667\n",
      "Epoch 5: Train Loss 0.2571, Val Loss 0.2975, Acc 0.8793, Prec 0.8874, Rec 0.8688, F1 0.8780\n",
      "Embeddings unfrozen.\n",
      "Epoch 6: Train Loss 0.2376, Val Loss 0.2804, Acc 0.8891, Prec 0.8755, Rec 0.9072, F1 0.8911\n",
      "Epoch 7: Train Loss 0.1317, Val Loss 0.3719, Acc 0.8794, Prec 0.9041, Rec 0.8488, F1 0.8756\n",
      "Epoch 8: Train Loss 0.0645, Val Loss 0.4972, Acc 0.8778, Prec 0.8522, Rec 0.9142, F1 0.8821\n",
      "Epoch 9: Train Loss 0.0460, Val Loss 0.5537, Acc 0.8850, Prec 0.8705, Rec 0.9046, F1 0.8872\n",
      "Epoch 10: Train Loss 0.0414, Val Loss 0.6300, Acc 0.8833, Prec 0.8611, Rec 0.9140, F1 0.8868\n",
      "Epoch 11: Train Loss 0.0388, Val Loss 0.6358, Acc 0.8844, Prec 0.8705, Rec 0.9032, F1 0.8865\n",
      "\n",
      "Training SemanticClassifierLSTM...\n",
      "Epoch 1: Train Loss 0.4077, Val Loss 0.3362, Acc 0.8515, Prec 0.8122, Rec 0.9144, F1 0.8603\n",
      "Epoch 2: Train Loss 0.3328, Val Loss 0.3092, Acc 0.8640, Prec 0.8289, Rec 0.9174, F1 0.8709\n",
      "Epoch 3: Train Loss 0.2969, Val Loss 0.3008, Acc 0.8725, Prec 0.8496, Rec 0.9052, F1 0.8765\n",
      "Epoch 4: Train Loss 0.2692, Val Loss 0.3059, Acc 0.8720, Prec 0.8991, Rec 0.8380, F1 0.8675\n",
      "Epoch 5: Train Loss 0.2434, Val Loss 0.3070, Acc 0.8722, Prec 0.8867, Rec 0.8534, F1 0.8698\n",
      "Embeddings unfrozen.\n",
      "Epoch 6: Train Loss 0.2240, Val Loss 0.2963, Acc 0.8799, Prec 0.8806, Rec 0.8790, F1 0.8798\n",
      "Epoch 7: Train Loss 0.1248, Val Loss 0.4061, Acc 0.8723, Prec 0.8611, Rec 0.8878, F1 0.8742\n",
      "Epoch 8: Train Loss 0.0667, Val Loss 0.4797, Acc 0.8790, Prec 0.8819, Rec 0.8752, F1 0.8785\n",
      "Epoch 9: Train Loss 0.0484, Val Loss 0.5756, Acc 0.8748, Prec 0.8611, Rec 0.8938, F1 0.8771\n",
      "Epoch 10: Train Loss 0.0391, Val Loss 0.6415, Acc 0.8732, Prec 0.8800, Rec 0.8642, F1 0.8720\n",
      "Epoch 11: Train Loss 0.0420, Val Loss 0.6861, Acc 0.8692, Prec 0.8377, Rec 0.9158, F1 0.8750\n",
      "\n",
      "Comparison of Final Test Results:\n",
      "Model\t\tAccuracy\tPrecision\tRecall\t\tF1\n",
      "SemanticClassifier\t0.8906\t\t0.8900\t\t0.8914\t\t0.8907\n",
      "SemanticClassifierGRU\t0.8891\t\t0.8755\t\t0.9072\t\t0.8911\n",
      "SemanticClassifierLSTM\t0.8799\t\t0.8806\t\t0.8790\t\t0.8798\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_and_evaluate_model(model_class, embedding_matrix, train_loader, test_loader):\n",
    "    # Hyperparameters for the model (optimizer, regularization, learning_rate)\n",
    "    model = model_class(embedding_matrix, hidden_dim=64, output_dim=1, dropout=0.7)  # Reduced hidden_dim, increased dropout\n",
    "    # Freeze embeddings initially\n",
    "    model.embedding.weight.requires_grad = False\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3)  # Increased weight_decay\n",
    "    criterion = nn.BCEWithLogitsLoss()  # More stable\n",
    "    patience = 0\n",
    "    best_f1 = 0  # Initialize best F1\n",
    "\n",
    "    # Training with 20 epochs\n",
    "    epochs = 20\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Unfreeze embeddings after 5 epochs\n",
    "        if epoch == 5:\n",
    "            model.embedding.weight.requires_grad = True\n",
    "            print(\"Embeddings unfrozen.\")\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                val_outputs = model(batch_X)\n",
    "                loss = criterion(val_outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                preds = (val_outputs > 0).int()  # Note: > 0 since logits, not probs\n",
    "                all_preds.extend(preds.numpy())\n",
    "                all_labels.extend(batch_y.int().numpy())\n",
    "        \n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss {train_loss/len(train_loader):.4f}, Val Loss {val_loss/len(test_loader):.4f}, Acc {acc:.4f}, Prec {prec:.4f}, Rec {rec:.4f}, F1 {f1:.4f}\")\n",
    "\n",
    "        # Early stopping based on F1\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            patience = 0\n",
    "            torch.save(model.state_dict(), f'best_{model_class.__name__}.pth')\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 5:\n",
    "                break\n",
    "\n",
    "    # Load best and evaluate\n",
    "    model.load_state_dict(torch.load(f'best_{model_class.__name__}.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_final_preds = []\n",
    "        all_final_labels = []\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            final_outputs = model(batch_X)\n",
    "            final_preds = (final_outputs > 0).int()\n",
    "            all_final_preds.extend(final_preds.numpy())\n",
    "            all_final_labels.extend(batch_y.int().numpy())\n",
    "        \n",
    "        final_acc = accuracy_score(all_final_labels, all_final_preds)\n",
    "        final_prec = precision_score(all_final_labels, all_final_preds, zero_division=0)\n",
    "        final_rec = recall_score(all_final_labels, all_final_preds, zero_division=0)\n",
    "        final_f1 = f1_score(all_final_labels, all_final_preds, zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': final_acc,\n",
    "        'precision': final_prec,\n",
    "        'recall': final_rec,\n",
    "        'f1': final_f1\n",
    "    }\n",
    "\n",
    "# Train and compare all models\n",
    "results = {}\n",
    "models = [SemanticClassifier, SemanticClassifierGRU, SemanticClassifierLSTM]\n",
    "\n",
    "for model_class in models:\n",
    "    print(f\"\\nTraining {model_class.__name__}...\")\n",
    "    results[model_class.__name__] = train_and_evaluate_model(model_class, embedding_matrix, train_loader, test_loader)\n",
    "\n",
    "print(\"\\nComparison of Final Test Results:\")\n",
    "print(\"Model\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF1\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}\\t{metrics['accuracy']:.4f}\\t\\t{metrics['precision']:.4f}\\t\\t{metrics['recall']:.4f}\\t\\t{metrics['f1']:.4f}\")\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
